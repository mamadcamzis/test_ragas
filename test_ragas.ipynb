{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "#from ragas.langchain import RagasEvaluatorChain\n",
    "# from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "\n",
    "from paths import get_paths\n",
    "from config import LlmParam\n",
    "#ChatParam\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_chains import kwf_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_FILE = \"llama-2-7b-chat.Q5_K_M.gguf\"\n",
    "MISTRAL_FILE = \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\"\n",
    "\n",
    "paths_dir = get_paths(model_file=LLAMA_FILE, model_name=\"LLAMA2\")\n",
    "mistral_paths = get_paths(model_file=MISTRAL_FILE, model_name=\"MISTRAL\")\n",
    "PROJECT_DIR = paths_dir[\"PROJECT_DIR\"]\n",
    "DATA_DIR = paths_dir[\"DATA_DIR\"]\n",
    "LLAMA_DIR = paths_dir[\"MODEL_DIR\"]\n",
    "MISTRAL_DIR = mistral_paths[\"MODEL_DIR\"]\n",
    "CHROMA_VECTOR_DIR = paths_dir[\"CHROMA_VECTOR_DIR\"]\n",
    "PREPARED_CHUNK_DIR = paths_dir[\"PREPARED_CHUNK_DIR\"]\n",
    "VECTOR_STORE = paths_dir[\"VECTOR_STORE\"]\n",
    "TEMPERATURE = LlmParam.TEMPERATURE\n",
    "N_GPU_LAYERS = LlmParam.N_GPU_LAYERS\n",
    "N_TOKENS = LlmParam.N_TOKENS\n",
    "KWF_TEMPLATE = LlmParam.KWF_TEMPLATE\n",
    "#MAX_CHAR_LIMIT = ChatParam.MAX_CHAR_LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = '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'\n",
    "key2 = 'sk-proj-4tKVWAEEpyYDkULwLxpFT3BlbkFJXGbkrYW3YMws0H0lH9dc'\n",
    "key3 = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = key3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_eval = '../data/baseEvaluation.xlsx'\n",
    "base_eval = '../data/BaseEvaluations.xlsx'\n",
    "eval_df = pd.read_excel(base_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire la ligne 4 à déplacer\n",
    "n = 3\n",
    "row_3 = eval_df.iloc[n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la ligne d'origine de l'index actuel\n",
    "eval_df = eval_df.drop(eval_df.index[n])\n",
    "\n",
    "# Réinsérer la ligne au début\n",
    "eval_df = pd.concat([row_3.to_frame().T, eval_df], ignore_index=True)\n",
    "\n",
    "# Afficher le DataFrame avec la ligne au début\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Question expert\"] = eval_df[\"Question expert\"].apply(lambda x: x.replace(\"CRPA\", \"code des relations entre le public et l’administration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Question novice\"] = eval_df[\"Question novice\"].apply(lambda x: x.replace(\"CRPA\", \"code des relations entre le public et l’administration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['Mots clés Question Expert'] = eval_df['Mots clés Question Expert'].apply(lambda x: x.replace(\"CRPA,\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Mots clés Question Novice\"] = eval_df['Mots clés Question Novice'].apply(lambda x: x.replace(\"CRPA,\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"code des relations entre le public et l’administration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Question expert\"] = eval_df[\"Question expert\"].astype(str)\n",
    "eval_df[\"Réponse expert\"] = eval_df[\"Réponse expert\"].astype(str)\n",
    "eval_df[\"Réponse Novice\"] = eval_df[\"Réponse Novice\"].astype(str)\n",
    "eval_df[\"Passage\"] = eval_df[\"Passage\"].astype(str)\n",
    "eval_df['Mots clés Question Expert'] = eval_df[\"Mots clés Question Expert\"].astype(str)\n",
    "eval_df[\"Mots clés Question Novice\"] = eval_df[\"Mots clés Question Novice\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec la métrique `answer_similarity` et `answer_correctness` de Ragas\n",
    "\n",
    "Le concept de similarité sémantique concerne l'évaluation de la ressemblance sémantique entre la réponse générée et la vraie réponse. Ce score est compris entre 0 et 1. Un score plus élevé signifie un meilleur alignement entre la réponse générée et la vraie réponse\n",
    "## Calcul \n",
    "Ce score se calcule en 3 étapes\n",
    "\n",
    "- Étape 1 : vectorisation de la réponse de référence à l'aide d'un modèle d'embedding.\n",
    "\n",
    "- Étape 2 : vectorisation de la réponse générée à l'aide du même embedding.\n",
    "\n",
    "- Étape 3 : calcul de la similarité cosinus entre les deux vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des questions expertes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression de question dont la réponse part dans tous les sens\n",
    "- Question 4: Quelles sont les mesures spécifiques requises par le CRPA pour la publication de documents contenant des données à caractère personnel, en particulier en ce qui concerne l'anonymisation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df[\"Question expert\"] = eval_df[\"Question expert\"].drop(index=3)\n",
    "# eval_df[\"Réponse expert\"]= eval_df[\"Réponse expert\"].drop(index=3)\n",
    "# eval_df[\"Passage\"] = eval_df[\"Passage\"].drop(index=3)\n",
    "# eval_df[\"Mots clés Question Expert\"] = eval_df[\"Mots clés Question Expert\"].drop(index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_experte = eval_df[\"Question expert\"] + eval_df[\"Mots clés Question Expert\"]\n",
    "reponses_experte = eval_df[\"Réponse expert\"]\n",
    "context = eval_df[\"Passage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Question_experte\"]  = eval_df[\"Question expert\"] + eval_df[\"Mots clés Question Expert\"]\n",
    "eval_df[\"Question_novice\"] = eval_df[\"Question novice\"] + eval_df[\"Mots clés Question Novice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_experte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions\n",
    "question_exp1 = questions_experte[0]\n",
    "question_exp2 = questions_experte[1] \n",
    "question_exp3 = questions_experte[2]\n",
    "question_exp4 = questions_experte[3]\n",
    "question_exp5 = questions_experte[4]\n",
    "question_exp6 = questions_experte[5]\n",
    "question_exp7 = questions_experte[6]\n",
    "question_exp8=  questions_experte[7] \n",
    "question_exp9 = questions_experte[8]\n",
    "# Réponses \n",
    "reponse_exp1 = reponses_experte[0]\n",
    "reponse_exp2 = reponses_experte[1] \n",
    "reponse_exp3 = reponses_experte[2]\n",
    "reponse_exp4 = reponses_experte[3]\n",
    "reponse_exp5 = reponses_experte[4]\n",
    "reponse_exp6 = reponses_experte[5]\n",
    "reponse_exp7 = reponses_experte[6]\n",
    "reponse_exp8=  reponses_experte[7] \n",
    "reponse_exp9 = questions_experte[8]\n",
    "# Contexte ou passage \n",
    "context_1 = context[0]\n",
    "context_2 = context[1] \n",
    "context_3 = context[2]\n",
    "context_4 = context[3]\n",
    "context_5 = context[4]\n",
    "context_6 = context[5]\n",
    "context_7 = context[6]\n",
    "context_8=  context[7] \n",
    "context_9 = context[8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# llm_model = LlamaCpp(\n",
    "#     model_path=LLAMA_DIR,\n",
    "#     n_gpu_layers=N_GPU_LAYERS,\n",
    "#     n_ctx=N_TOKENS,\n",
    "#     n_batch=512,\n",
    "#     f16_kv=True,\n",
    "#     temperature=TEMPERATURE,\n",
    "#     max_tokens=N_TOKENS,\n",
    "#     n_parts=1,\n",
    "#     verbose=True,\n",
    "#     echo=False,\n",
    "#     streaming=True,\n",
    "#     callback_manager=callback_manager,\n",
    "# )\n",
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\",\n",
    "#     input_key=\"question\",\n",
    "#     return_messages=True,\n",
    "#     output_key=\"answer\",\n",
    "# )\n",
    "# prompt = PromptTemplate(\n",
    "# \t\t\t    input_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "# \t\t\t    template=KWF_TEMPLATE,\n",
    "# \t\t\t)\n",
    "# embedding = HuggingFaceInstructEmbeddings(\n",
    "#         model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Question_experte\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df = eval_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_expert = eval_df[[\"Question_experte\", \"Passage\", \"Réponse expert\"]]\n",
    "ground_truth_expert = ground_truth_expert.rename(columns={\"Question_experte\": \"question\", \"Passage\": \"context\",\n",
    "                                                          \"Réponse expert\" : \"ground_truth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_novice = eval_df[[\"Question_novice\", \"Passage\", \"Réponse Novice\"]]\n",
    "ground_truth_novice = ground_truth_novice.rename(columns={\"Question_novice\": \"question\", \"Passage\": \"context\",\n",
    "                                                          \"Réponse Novice\" : \"ground_truth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_expert = Dataset.from_pandas(ground_truth_expert)\n",
    "eval_dataset_novice = Dataset.from_pandas(ground_truth_novice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_expert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def create_ragas_dataset(eval_dataset):\n",
    "rag_dataset = []\n",
    "i = 0\n",
    "#for question in questions_experte:\n",
    "for row in tqdm(eval_dataset_expert):\n",
    "\t\ti = i+1\n",
    "\t\tprint(row)\n",
    "\t\t#question = question_exp4\n",
    "\t\t# \n",
    "\t\tquestion = row[\"question\"]\n",
    "\t\tprint(f\"Traitement de la question {i}: {question} ...\")\n",
    "\t\t#question = str(question)\n",
    "\t\t# \n",
    "\t\tkeyword_part = question.split(\"?\")[-1]\n",
    "\t\tlogging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "\t\tmemory = ConversationBufferMemory(\n",
    "\t\t\t\tmemory_key=\"chat_history\",\n",
    "\t\t\t\tinput_key=\"question\",\n",
    "\t\t\t\treturn_messages=True,\n",
    "\t\t\t\toutput_key=\"answer\")\n",
    "\t\tprompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t\t\t)\n",
    "\t\tchain = kwf_chain(\n",
    "\t\t\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\t\t\tPREPARED_CHUNK_DIR,\n",
    "\t\t)\n",
    "\t\tinput_structure = {\"question\": question}\n",
    "\n",
    "\t\tanswer = chain.invoke(input_structure)\n",
    "\t\textraits = []\n",
    "\t\tfor i in range(min(3, len(answer[\"source_documents\"]))):\n",
    "\t\t\textraits.append(answer[\"source_documents\"][i].page_content)\n",
    "\t\t\n",
    "\t\trag_dataset.append(\n",
    "\t\t\t\t\t\t\t\t\t{\"question\" : question,\n",
    "\t\t\t\t\t\t\t\t\t\t\"answer\" : answer[\"answer\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\"contexts\" : [extrait for extrait in extraits],\n",
    "\t\t\t\t\t\t\t\t\t\t\"ground_truth\" : row[\"ground_truth\"]\n",
    "\t\t\t\t\t\t\t\t\t\t})\n",
    "\t\t\n",
    "\n",
    "# answers.append(response[\"answer\"])\n",
    "\n",
    "# \t#exit()\n",
    "# \t#chunk[\"source_documents\"][i].page_content\n",
    "# contexts.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_exp = pd.DataFrame(rag_dataset)\n",
    "rag_eval_dataset_expert = Dataset.from_pandas(rag_df_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_exp.to_parquet(\"../ragasDataset/EvaluationExpert_temp_{TEMPERATURE}.pq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rag_df_exp.to_parquet(\"../ragasDataset/dataExpert.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_exp = pd.read_parquet(\"../ragasDataset/dataExpert.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset_expert = Dataset.from_pandas(rag_df_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hg = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ragas_dataset(rg_dataset, metrics):\n",
    "  result = evaluate(\n",
    "    rg_dataset,\n",
    "    metrics=metrics\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rag_df_exp.to_csv(f\"../ragasDataset/EvaluationExpert_temp_{TEMPERATURE}.csv\", index=False)\n",
    "metric1 = [answer_similarity, faithfulness, answer_relevancy, context_relevancy, answer_correctness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity_2  = evaluate_ragas_dataset(rag_eval_dataset_expert, metrics=metric1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity_2_df = results_answer_similarity_2.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity = results_answer_similarity.to_pandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity[\"answer_similarity\"] = results_answer_similarity[\"answer_similarity\"].map(lambda x: str(round(int(x) / 100, 1)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity['answer_similarity'] = results_answer_similarity['answer_similarity'].str.replace(\"%\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity['answer_similarity'] = results_answer_similarity['answer_similarity'].apply(lambda x : str(int(x)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity['answer_similarity'] = results_answer_similarity['answer_similarity'].str.replace(\"0\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity.to_excel(\"QuestionsExpertes.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_exp = pd.read_csv(\"../ragasDataset/EvaluationExpert_temp_0.01.csv\")\n",
    "rag_eval_dataset_expert = Dataset.from_pandas(rag_df_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\t# for row in tqdm(eval_dataset):\n",
    "\t#   answer = rag_pipeline.invoke({\"question\" : row[\"Question_experte\"]})\n",
    "\t#   rag_dataset.append(\n",
    "\t#       {\"question\" : row[\"question\"],\n",
    "\t#        \"answer\" : answer[\"response\"],\n",
    "\t#        \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "\t#        \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "\t#        }\n",
    "\t#   )\n",
    "# answers = []\n",
    "# contexts = []\n",
    "#def create_ragas_dataset(eval_dataset):\n",
    "rag_dataset = []\n",
    "i = 0\n",
    "#for question in questions_experte:\n",
    "for row in tqdm(eval_dataset_novice):\n",
    "\t\ti = i+1\n",
    "\t\tprint(row)\n",
    "\t\t#question = question_exp4\n",
    "\t\t# \n",
    "\t\tquestion = row[\"question\"]\n",
    "\t\tprint(f\"Traitement de la question {i}: {question} ...\")\n",
    "\t\t#question = str(question)\n",
    "\t\t# \n",
    "\t\tkeyword_part = question.split(\"?\")[-1]\n",
    "\t\tlogging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "\t\tmemory = ConversationBufferMemory(\n",
    "\t\t\t\tmemory_key=\"chat_history\",\n",
    "\t\t\t\tinput_key=\"question\",\n",
    "\t\t\t\treturn_messages=True,\n",
    "\t\t\t\toutput_key=\"answer\")\n",
    "\t\tprompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t\t\t)\n",
    "\t\tchain = kwf_chain(\n",
    "\t\t\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\t\t\tPREPARED_CHUNK_DIR,\n",
    "\t\t)\n",
    "\t\tinput_structure = {\"question\": question}\n",
    "\n",
    "\t\tanswer = chain.invoke(input_structure)\n",
    "\t\textraits = []\n",
    "\t\tfor i in range(min(3, len(answer[\"source_documents\"]))):\n",
    "\t\t\textraits.append(answer[\"source_documents\"][i].page_content)\n",
    "\t\t\n",
    "\t\trag_dataset.append(\n",
    "\t\t\t\t\t\t\t\t\t{\"question\" : question,\n",
    "\t\t\t\t\t\t\t\t\t\t\"answer\" : answer[\"answer\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\"contexts\" : [extrait for extrait in extraits],\n",
    "\t\t\t\t\t\t\t\t\t\t\"ground_truth\" : row[\"ground_truth\"]\n",
    "\t\t\t\t\t\t\t\t\t\t})\n",
    "\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_nov = pd.DataFrame(rag_dataset)\n",
    "rag_eval_dataset_novice = Dataset.from_pandas(rag_df_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [answer_similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity_novice = Dataset.from_dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rag_eval_dataset_novice[0]['answer'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_eval = []\n",
    "rag_df_nov = pd.read_parquet(f\"../ragasDataset/EvaluationNovice_temp_{TEMPERATURE}.pq\")\n",
    "rag_eval_dataset_novice = Dataset.from_pandas(rag_df_nov)\n",
    "# for row in rag_eval_dataset_novice:\n",
    "#  #print(type(row))\n",
    "#  #row['answer'] = row['answer'].strip()\n",
    "#  df = pd.DataFrame.from_dict(row)\n",
    "#  eval_i = Dataset.from_pandas(df)\n",
    "#  #print(\n",
    " # \n",
    "evaluate_ragas_dataset(rag_eval_dataset_novice, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset_novice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity_novice_q1 = results_answer_similarity_novice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_nov.to_csv(f\"../ragasDataset/EvaluationNovice_temp_{TEMPERATURE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df_nov.to_parquet(\"../ragasDataset/EvaluationNovice_temp_{TEMPERATURE}.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_df = pd.DataFrame(rag_dataset)\n",
    "# rag_eval_dataset = Dataset.from_pandas(rag_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-4tKVWAEEpyYDkULwLxpFT3BlbkFJXGbkrYW3YMws0H0lH9dc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = [\n",
    "     \n",
    "#      \t,\n",
    "       \n",
    "        \n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTS = [context.split('\\\\n.') for context in rag_eval_dataset_expert[\"contexts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset_expert[\"contexts\"] = CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_relevancy  = evaluate_ragas_dataset(rag_eval_dataset_expert, metrics=[answer_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_context_relevancy  = evaluate_ragas_dataset(rag_eval_dataset_expert, metrics=[context_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_correctness  = evaluate_ragas_dataset(rag_eval_dataset_expert, metrics=[answer_correctness]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_faithfull  = evaluate_ragas_dataset(rag_eval_dataset_expert, metrics=[faithfulness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_faithfullness_df = results_faithfull.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_df = pd.read_csv(f\"DatasetEvaluation_temp_{TEMPERATURE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reponses_experte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df  = Dataset.from_dict({\"question\": questions_experte, \n",
    "                             \"answer\": answers,\n",
    "                             \"contexts\": contexts,\n",
    "                             \"ground_truth\": reponses_experte\n",
    "                             })\n",
    "#response_df= Dataset.from_dict(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = eval_df['contexts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT2 = [context.split('\\\\n')  for context in CONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_df  = Dataset.from_dict({\"question\": questions_experte, \n",
    "#                              \"answer\": answers,\n",
    "#                              \"contexts\": contexts,\n",
    "#                              \"ground_truth\": reponses_experte\n",
    "#                              })\n",
    "# #response_df = Dataset.from_dict(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_df = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import RunConfig\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ragas_dataset(rg_dataset, metrics):\n",
    "    \n",
    "    \n",
    "    result = evaluate(\n",
    "        rg_dataset,\n",
    "        metrics=metrics,\n",
    "       \n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "metrics = [answer_similarity]\n",
    "rag_df_exp = pd.read_parquet(\"/home/userds/projet-rag/chatbot/ragasDataset/dataExpert.pq\")\n",
    "rag_eval_dataset_expert = Dataset.from_pandas(rag_df_exp)\n",
    "answer_sim = [answer_similarity]\n",
    "eval_dataset  = evaluate_ragas_dataset(rag_eval_dataset_expert, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "metrics = [answer_similarity]\n",
    "rag_df_novice = pd.read_parquet(\"/home/userds/projet-rag/chatbot/ragasDataset/EvaluationNovice_temp_0.01.pq\")\n",
    "rag_eval_dataset_novice = Dataset.from_pandas(rag_df_novice)\n",
    "#answer_sim = [answer_similarity]\n",
    "eval_dataset  = evaluate_ragas_dataset(rag_eval_dataset_novice, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [answer_similarity]\n",
    "\n",
    "# answer_correctness,\n",
    "# faithfulness,\n",
    "# answer_relevancy,\n",
    "# context_relevancy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_similarity = evaluate(response_df,  metrics=metrics, embeddings=embedding_hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_faithfulness = evaluate(response_df,  metrics=[faithfulness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity = results_similarity.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity.to_csv(\"Answer_similarity_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_faithfull = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_faithfull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_answer_similarity.to_csv(f\"Evaluations_temperature{TEMPERATURE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# define llm and embeddings\n",
    "#langchain_llm = BaseLanguageModel(model=\"my_model\") # any langchain LLM instance\n",
    "langchain_embeddings = Embeddings(model=embedding_hg) # any langchain Embeddings instance\n",
    "\n",
    "# make sure to wrap them with wrappers\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "#langchain_llm = LangchainLLMWrapper(langchain_llm)\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(langchain_embeddings)\n",
    "\n",
    "# you can also use custom LLMs and Embeddings here but make sure \n",
    "# # they are subclasses of BaseRagasLLM and BaseRagasEmbeddings\n",
    "# llm = MyCustomLLM()\n",
    "# embeddings = MyCustomEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_4 = []\n",
    "contexts_4 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp4\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_4 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_4.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_1 = []\n",
    "contexts_1 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp1\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_1 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_1.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contexte_1 = [['\\n. QUELLES SONT LES OBLIGATIONS DE PUBLICATION EN LIGNE ?. • les administrations ne sont pas tenues de publier les archives publiques issues des opérations de sélection prévues aux articles L. 212-2 et L. 212-3 du code du patrimoine. Attention : les obligations de publication en ligne telles que détaillées ci-après n’impliquent pas une diffusion des documents en question dans leur intégralité. En effet les administrations seront tenues de s’interroger sur la nécessité d’occulter certaines informations non communicables ou d’anonymiser le document ( cf 2. Infra). Les obligations prévues par le code des relations entre le public et l’administration L’article L. 312-1 du CRPA consacre de façon générale la faculté pour les administrations de publier les documents administratifs qu’elles produisent ou reçoivent. Le CRPA prévoit également des obligations légales de publication.',\n",
    "  '\\n. QUELLES SONT LES OBLIGATIONS. \\nDE PUBLICATION EN LIGNE ?. Articles L. 300-2 et L. 300-3 du CRPA. À noter que : • seuls les documents qui ont un lien suffisamment direct avec l’exercice des missions de service public, ou qui retracent les conditions dans lesquelles l’organisme privé exerce sa mission de service public, ont un caractère administratif, à l’exclusion des documents relatifs uniquement au fonctionnement interne de cet organisme5 ; Toutefois, bien qu’ils soient sans lien avec les missions de service public, les documents relatifs à la gestion du domaine privé des personnes publiques sont désormais soumis au régime d’accès des documents administratifs. • le document doit exister à la date de la demande. Lorsqu’il n’existe pas en tant que tel, le document doit pouvoir être créé par un traitement automatisé d’usage courant (c’est-à-dire en ayant recours à un programme informatique de maniement aisé et à la disposition du service qui détient la base de données) ; En d’autres termes, l’administration n’est pas tenue, sauf disposition particulière, d’élaborer un document pour répondre à une demande6, ni de numériser un document dont elle dispose exclusivement sous forme papier afin de le mettre en ligne. Cependant, la création d’un document peut être imposée à l’administration lorsqu’une obligation de publication implique la création d’un document ad hoc. C’est le cas par exemple de la publication des règles définissant les principaux traitements algorithmiques (cf 1.1.2.2.) ou en matière d’information environnementale.',\n",
    "  '\\n. QUELLES SONT LES OBLIGATIONS. \\nDE PUBLICATION EN LIGNE ?. Articles L. 300-2 et L. 300-3 du CRPA. Ces obligations concernent les documents administratifs au sens du CRPA, c’est-à-dire tout document produit ou reçu par l’administration au sens de l’article L. 300-2 du code des relations entre le public et l’administration dans le cadre d’une mission de service public. Les documents administratifs peuvent revêtir de nombreuses formes (dossiers, rapports, études, comptes rendus, procès-verbaux, statistiques, directives, instructions, circulaires, codes sources, etc.) et adopter tout support (écrit, enregistrement sonore ou visuel, forme numérique ou informatique). Il peut s’agir de documents détenus par l’État, les collectivités territoriales mais aussi par les autres personnes de droit public ou les personnes de droit privé. Pour déterminer si une personne privée est chargée d’une mission de service public, dans le silence des textes, il y a lieu de contrôler4 : • si la personne privée assure une mission d’intérêt général sous le contrôle de l’administration et si elle est dotée à cette fin de prérogatives de puissance publique ; • ou, en l’absence de telles prérogatives, si, eu égard à l’intérêt général de son activité, aux conditions de sa création, de son organisation ou de son fonctionnement, aux obligations qui lui sont imposées ainsi qu’aux mesures prises pour vérifier que les objectifs qui lui sont assignés sont atteints, il apparaît que l’administration a entendu lui confier une telle mission.',\n",
    "  '46 https://references.modernisation.gouv.fr/sites/default/files/Referentiel_General_Interoperabilite_V2.pdf. \\n. 4 COMMENT RÉUTILISER LES DONNÉES DIFFUSÉES ?. Les obligations prévues par le CRPA pour la réutilisation. Articles L. 321-1 et suivants du CRPA. Par principe, les informations publiques figurant dans des documents communiqués ou publiés par les administrations48 peuvent être utilisées par toute personne (physique ou morale, publique ou privée) qui le souhaite à d’autres fins que celles de la mission de service public pour les besoins de laquelle les documents ont été produits ou reçus. Sont donc librement réutilisables : • les données figurant dans des documents publiés par l’administration ; • les données figurant dans des documents communiqués par l’administration ou pour lesquels la communication est un droit, sous réserve du respect d’éventuelles obligations d’occultation (voir supra). La réutilisation des données ne dépend pas du régime juridique sous lequel s’est effectuée leur communication ou la publication (CRPA ou autres obligations légales ou règlementaires). Certains obstacles peuvent cependant restreindre la libre réutilisation : • les droits de propriété intellectuelle détenus par des tiers à l’administration ainsi que ceux des administrations sur les bases de données qu’elles ont produites ou reçues dans l’exercice d’une mission de service public à caractère industriel ou commercial soumise à la concurrence. Les autres administrations, ne peuvent en revanche, au titre des droits qu’elles détiennent au titre des articles L. 342-1 et L. 342-2 du code de la propriété intellectuelle, faire obstacle à la réutilisation du contenu des bases de données qu’elles publient en application du 3° de l’article L. 312-1-1 du présent code dans le cadre de l’« open data »49.',\n",
    "  '57 https://www.cnil.fr/fr/les-droits-pour-maitriser-vos-donnees-personnelles. 58 https://www.cnil.fr/fr/limiter-la-conservation-des-donnees. 59 https://www.cnil.fr/fr/garantir-la-securite-des-donnees. \\n. GLOSSAIRE. Définitions. Anonymisation : souvent confondue avec la notion de pseudonymisation, l’anonymisation est un traitement effectué sur un document permettant de rendre impossible par quelque moyen que ce soit, et cela de façon irréversible, l’identification des personnes. Lorsque l’anonymisation est effective, le RGPD ne s’applique plus au traitement des données, celles-ci n’étant dès lors plus à caractère personnel. Archive publique : ensemble des documents, y compris les données, quels que soient leur date, leur lieu de conservation, leur forme et leur support, produits ou reçus par toute personne physique ou morale et par tout service ou organisme public ou privé dans l’exercice d’une activité de service public. Ainsi, tout document administratif est une archive publique. Les archives publiques forment toutefois un ensemble plus vaste que les documents administratifs dans la mesure où elles englobent les documents qui sont exclus du champ d’application du CRPA, notamment les documents juridictionnels et judiciaires. Les archives du Conseil constitutionnel et des assemblées parlementaires ne sont pas régies par le régime de droit commun des archives publiques mais par des dispositions qui sont propres à chacune de ces instances.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_2 = []\n",
    "contexts_2 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp2\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_2 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_2 = contexts_2.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_3 = []\n",
    "contexts_3 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp3\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_3 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_3.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_5 = []\n",
    "contexts_5 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp5\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_5 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_5.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_6 = []\n",
    "contexts_6 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp6\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_6 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_6.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_7 = []\n",
    "contexts_7 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp7\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_7 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_7.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_8 = []\n",
    "contexts_8 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp8\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_8 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_8.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_9 = []\n",
    "contexts_9 = []\n",
    "#i = 0\n",
    "# for question in questions_experte:\n",
    "# i = i+1\n",
    "# if i <=10:\n",
    "question = question_exp9\n",
    "print(f\"Traitement de la question {question} ...\")\n",
    "#question = str(question)\n",
    "keyword_part = question.split(\"?\")[-1]\n",
    "logging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\",\n",
    "\t\tinput_key=\"question\",\n",
    "\t\treturn_messages=True,\n",
    "\t\toutput_key=\"answer\")\n",
    "prompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t)\n",
    "chain = kwf_chain(\n",
    "\t\t\t\tkeyword_part, prompt, memory,\n",
    "\t\t\t\tPREPARED_CHUNK_DIR,\n",
    ")\n",
    "input_structure = {\"question\": question}\n",
    "\n",
    "response = chain.invoke(input_structure)\n",
    "answers_9 = response[\"answer\"]\n",
    "\n",
    "\t#exit()\n",
    "\t#chunk[\"source_documents\"][i].page_content\n",
    "contexts_9.append([context.page_content for context in response[\"source_documents\"]])\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "Part dans tous les sens des fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "# import InstructorEmbedding\n",
    "# embedding = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_experte = [question_exp1, question_exp2, question_exp3, question_exp4, question_exp5,\n",
    "                     question_exp6, question_exp7, question_exp8, question_exp9]\n",
    "answers = [answers_1, answers_2, answers_3, answers_4, answers_5, \n",
    "           answers_6, answers_7, answers_8, answers_9]\n",
    "contexts = [contexts_1, contexts_2, contexts_3, contexts_4, contexts_5, \n",
    "            contexts_6, contexts_7, contexts_8, contexts_9]\n",
    "reponses_experte = [reponse_exp1, reponse_exp2, reponse_exp3, reponse_exp4, reponse_exp5,\n",
    "                    reponse_exp6, reponse_exp7, reponse_exp8, reponse_exp9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataframe = pd.read_csv(f\"DatasetEvaluation_temp_{TEMPERATURE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context11 = [[['\\n. QUELLES SONT LES OBLIGATIONS DE PUBLICATION EN LIGNE ?. • les administrations ne sont pas tenues de publier les archives publiques issues des opérations de sélection prévues aux articles L. 212-2 et L. 212-3 du code du patrimoine. Attention : les obligations de publication en ligne telles que détaillées ci-après n’impliquent pas une diffusion des documents en question dans leur intégralité. En effet les administrations seront tenues de s’interroger sur la nécessité d’occulter certaines informations non communicables ou d’anonymiser le document ( cf 2. Infra). Les obligations prévues par le code des relations entre le public et l’administration L’article L. 312-1 du CRPA consacre de façon générale la faculté pour les administrations de publier les documents administratifs qu’elles produisent ou reçoivent. Le CRPA prévoit également des obligations légales de publication.',\n",
    "  '\\n. QUELLES SONT LES OBLIGATIONS. \\nDE PUBLICATION EN LIGNE ?. Articles L. 300-2 et L. 300-3 du CRPA. À noter que : • seuls les documents qui ont un lien suffisamment direct avec l’exercice des missions de service public, ou qui retracent les conditions dans lesquelles l’organisme privé exerce sa mission de service public, ont un caractère administratif, à l’exclusion des documents relatifs uniquement au fonctionnement interne de cet organisme5 ; Toutefois, bien qu’ils soient sans lien avec les missions de service public, les documents relatifs à la gestion du domaine privé des personnes publiques sont désormais soumis au régime d’accès des documents administratifs. • le document doit exister à la date de la demande. Lorsqu’il n’existe pas en tant que tel, le document doit pouvoir être créé par un traitement automatisé d’usage courant (c’est-à-dire en ayant recours à un programme informatique de maniement aisé et à la disposition du service qui détient la base de données) ; En d’autres termes, l’administration n’est pas tenue, sauf disposition particulière, d’élaborer un document pour répondre à une demande6, ni de numériser un document dont elle dispose exclusivement sous forme papier afin de le mettre en ligne. Cependant, la création d’un document peut être imposée à l’administration lorsqu’une obligation de publication implique la création d’un document ad hoc. C’est le cas par exemple de la publication des règles définissant les principaux traitements algorithmiques (cf 1.1.2.2.) ou en matière d’information environnementale.',\n",
    "  '\\n. QUELLES SONT LES OBLIGATIONS. \\nDE PUBLICATION EN LIGNE ?. Articles L. 300-2 et L. 300-3 du CRPA. Ces obligations concernent les documents administratifs au sens du CRPA, c’est-à-dire tout document produit ou reçu par l’administration au sens de l’article L. 300-2 du code des relations entre le public et l’administration dans le cadre d’une mission de service public. Les documents administratifs peuvent revêtir de nombreuses formes (dossiers, rapports, études, comptes rendus, procès-verbaux, statistiques, directives, instructions, circulaires, codes sources, etc.) et adopter tout support (écrit, enregistrement sonore ou visuel, forme numérique ou informatique). Il peut s’agir de documents détenus par l’État, les collectivités territoriales mais aussi par les autres personnes de droit public ou les personnes de droit privé. Pour déterminer si une personne privée est chargée d’une mission de service public, dans le silence des textes, il y a lieu de contrôler4 : • si la personne privée assure une mission d’intérêt général sous le contrôle de l’administration et si elle est dotée à cette fin de prérogatives de puissance publique ; • ou, en l’absence de telles prérogatives, si, eu égard à l’intérêt général de son activité, aux conditions de sa création, de son organisation ou de son fonctionnement, aux obligations qui lui sont imposées ainsi qu’aux mesures prises pour vérifier que les objectifs qui lui sont assignés sont atteints, il apparaît que l’administration a entendu lui confier une telle mission.',\n",
    "  '46 https://references.modernisation.gouv.fr/sites/default/files/Referentiel_General_Interoperabilite_V2.pdf. \\n. 4 COMMENT RÉUTILISER LES DONNÉES DIFFUSÉES ?. Les obligations prévues par le CRPA pour la réutilisation. Articles L. 321-1 et suivants du CRPA. Par principe, les informations publiques figurant dans des documents communiqués ou publiés par les administrations48 peuvent être utilisées par toute personne (physique ou morale, publique ou privée) qui le souhaite à d’autres fins que celles de la mission de service public pour les besoins de laquelle les documents ont été produits ou reçus. Sont donc librement réutilisables : • les données figurant dans des documents publiés par l’administration ; • les données figurant dans des documents communiqués par l’administration ou pour lesquels la communication est un droit, sous réserve du respect d’éventuelles obligations d’occultation (voir supra). La réutilisation des données ne dépend pas du régime juridique sous lequel s’est effectuée leur communication ou la publication (CRPA ou autres obligations légales ou règlementaires). Certains obstacles peuvent cependant restreindre la libre réutilisation : • les droits de propriété intellectuelle détenus par des tiers à l’administration ainsi que ceux des administrations sur les bases de données qu’elles ont produites ou reçues dans l’exercice d’une mission de service public à caractère industriel ou commercial soumise à la concurrence. Les autres administrations, ne peuvent en revanche, au titre des droits qu’elles détiennent au titre des articles L. 342-1 et L. 342-2 du code de la propriété intellectuelle, faire obstacle à la réutilisation du contenu des bases de données qu’elles publient en application du 3° de l’article L. 312-1-1 du présent code dans le cadre de l’« open data »49.',\n",
    "  '57 https://www.cnil.fr/fr/les-droits-pour-maitriser-vos-donnees-personnelles. 58 https://www.cnil.fr/fr/limiter-la-conservation-des-donnees. 59 https://www.cnil.fr/fr/garantir-la-securite-des-donnees. \\n. GLOSSAIRE. Définitions. Anonymisation : souvent confondue avec la notion de pseudonymisation, l’anonymisation est un traitement effectué sur un document permettant de rendre impossible par quelque moyen que ce soit, et cela de façon irréversible, l’identification des personnes. Lorsque l’anonymisation est effective, le RGPD ne s’applique plus au traitement des données, celles-ci n’étant dès lors plus à caractère personnel. Archive publique : ensemble des documents, y compris les données, quels que soient leur date, leur lieu de conservation, leur forme et leur support, produits ou reçus par toute personne physique ou morale et par tout service ou organisme public ou privé dans l’exercice d’une activité de service public. Ainsi, tout document administratif est une archive publique. Les archives publiques forment toutefois un ensemble plus vaste que les documents administratifs dans la mesure où elles englobent les documents qui sont exclus du champ d’application du CRPA, notamment les documents juridictionnels et judiciaires. Les archives du Conseil constitutionnel et des assemblées parlementaires ne sont pas régies par le régime de droit commun des archives publiques mais par des dispositions qui sont propres à chacune de ces instances.']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataframe.loc[1, [\"contexts\"]] = context11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    \"question\" : eval_dataframe[\"question\"].values.flatten(),\n",
    "    \"answer\" : eval_dataframe[\"answer\"].values.flatten(),\n",
    "    \"contexts\" : eval_dataframe[\"contexts\"].values.flatten(),\n",
    "    \"ground_truth\" : eval_dataframe[\"ground_truth\"].values.flatten(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_df  = Dataset.from_dict({\n",
    "#     \"question\" : eval_dataframe[\"question\"],\n",
    "#     \"answer\" : eval_dataframe[\"answer\"],\n",
    "#     \"contexts\" : eval_dataframe[\"contexts\"],\n",
    "#     \"ground_truth\" : eval_dataframe[\"ground_truth\"],\n",
    "# })\n",
    "response_df = Dataset.from_dict(my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [answer_similarity,\n",
    "answer_correctness,\n",
    "faithfulness,\n",
    "answer_relevancy,\n",
    "context_relevancy]\n",
    "embedding = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"}\n",
    "    )\n",
    "results = evaluate(eval_dataframe,  metrics=metrics, embeddings=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.to_pandas()\n",
    "\n",
    "results_df.to_excel(f\"QAExpert_temperature_{TEMPERATURE}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df[\"ground_truth\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = df.ground.values[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"question\", \"ground_truth\", \"answer\", \"answer_similarity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des questions novices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_novice = eval_df[\"Question novice\"] + eval_df[\"Mots clés Question Novice\"]\n",
    "reponses_novice = eval_df[\"Réponse Novice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Answer_similarity\n",
    "- 5 Exemples\n",
    "- 1 Exemple  avec les autres critères RAG TRiad\n",
    "- Définition des métriques\n",
    "- Cosine Similarity de langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test d'évaluation RAG TRIAD \n",
    "\n",
    "\n",
    "\n",
    "`Faithfulness ou Groundness`:  Consistance de la réponse générée par rapport au contexte. Le calcul se fait entre la réponse générée par le LLM et le contexte retriévié. Une réponse est dite faithfull si toutes les affirmations qui sont dans la réponse peuvent être prédites par le contexte retriévié. \n",
    "\n",
    "Pour ce faire, on identifie d'abord un ensemble d'affirmation sur la réponse, et on quantifie si l'affirmation peut être prédite par le contexte retriévie. Le groundness est la moyenne de toutes les affirmations \n",
    "\n",
    "__Exemple__ :  \n",
    "\n",
    "- Question : Qui est Albert Einstein ? \n",
    "\n",
    "- Contexte : Albert Einstein (né le 14 mars 1879) est un physicien théoricien d'origine allemande, largement considéré comme l'un des scientifiques les plus importants et les plus influents de tous les temps. \n",
    "\n",
    "- Réponse avec haut faithfulness : Einstein est né en Allemagne le 14 mars 1879. \n",
    "\n",
    "- Réponse avec faible faithfulness : Einstein est né en Allemagne le 20 mars 1879. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answer relevance` \n",
    "\n",
    "Cette métrique mesure la pertinence de la réponse générée par rapport au prompt (question). Cette mesure est calculée à partir de la question, du contexte et de la réponse. \n",
    "\n",
    "__Exemple__ : \n",
    "\n",
    "**Question** : Où se trouve la France et quelle est sa capitale ? \n",
    "\n",
    "**Réponse peu pertinente** : La France est située en Europe occidentale. \n",
    "\n",
    "**Réponse très pertinente** : La France est située en Europe occidentale et Paris est sa capitale. \n",
    "\n",
    "__Méthode de calcul__ : \n",
    "\n",
    "Pour calculer la pertinence de la réponse à une question donnée, nous suivons deux étapes : \n",
    "\n",
    "- Étape 1 : rétro-ingénierie de \"n\" variantes de la question à partir de la réponse générée à l'aide d'un grand modèle linguistique (LLM). Par exemple, pour la première réponse, le LLM peut générer les questions possibles suivantes :  \n",
    "\n",
    "\t\t- Question 1 : \"Dans quelle partie de l'Europe se trouve la France ?\" \n",
    "\n",
    "\t\t- Question 2 : \"Quelle est la situation géographique de la France en Europe ?\" \n",
    "\n",
    "\t\t- Question 3 : \"Pouvez-vous identifier la région d'Europe où se trouve la France ?\" \n",
    "\n",
    "- Étape 2 : Calculer la similarité moyenne en cosinus entre les questions générées et la question réelle. \n",
    "\n",
    "Le concept sous-jacent est que si la réponse répond correctement à la question, il est très probable que la question originale puisse être reconstruite uniquement à partir de la réponse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Contexte relevance` \n",
    "\n",
    "Cette mesure évalue la pertinence du contexte retrouvé. Le calcul se fait sur la base de la question et des contextes. Les valeurs sont comprises entre 0 et 1, les valeurs les plus élevées indiquant une meilleure pertinence. \n",
    "\n",
    "  \n",
    "\n",
    "Idéalement, le contexte récupéré devrait contenir exclusivement des informations essentielles pour répondre à la requête fournie. Pour ce faire, nous estimons d'abord la valeur en identifiant les phrases du contexte retrouvé qui sont pertinentes pour répondre à la question posée.  \n",
    "\n",
    "__Exemple__: \n",
    "\n",
    "- Question : Quelle est la capitale de la France ? \n",
    "\n",
    "- Contexte très pertinent : La France, située en Europe occidentale, comprend des villes médiévales, des villages alpins et des plages méditerranéennes. Paris, sa capitale, est célèbre pour ses maisons de couture, ses musées d'art classique, dont le Louvre, et ses monuments comme la tour Eiffel. \n",
    "\n",
    "- Faible pertinence du contexte : La France, située en Europe occidentale, comprend des villes médiévales, des villages alpins et des plages méditerranéennes. Paris, sa capitale, est célèbre pour ses maisons de couture, ses musées d'art classique, dont le Louvre, et ses monuments comme la Tour Eiffel. Le pays est également réputé pour ses vins et sa cuisine raffinée. Les dessins rupestres de Lascaux, le théâtre romain de Lyon et le vaste château de Versailles témoignent de la richesse de son histoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset_df = response_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset_df = small_dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = Dataset.from_pandas(small_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import context_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    answer_correctness\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(small_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarité cosinus avec LangChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine = pd.DataFrame.from_dict(response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import InstructorEmbedding\n",
    "embedding = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_embeddings = embedding.embed_documents(df_cosine[\"ground_truth\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_embeddings = embedding.embed_documents(df_cosine[\"answer\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(true_embeddings, answers_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(results_df[[\"answer_relevancy\", \"answer_similarity\"]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################### Test avec RAGAS ###########################\n",
    "# make eval chains\n",
    "# eval_chains = {\n",
    "#     m.name: RagasEvaluatorChain(\n",
    "#         metric=m\n",
    "#     )\n",
    "#     for m in [faithfulness, answer_relevancy, context_relevancy, context_recall]\n",
    "# }\n",
    "\n",
    "# # evaluate\n",
    "# for name, eval_chain in eval_chains.items():\n",
    "#     print(f\" Name {name} ...\")\n",
    "#     score_name = f\"{name}_score\"\n",
    "#     print(f\"{score_name}: Eval chain type: {type(eval_chain)} \\n\")\n",
    "#     print(f\"{score_name}: {eval_chain(answers)[score_name]}\")\n",
    "# export LANGCHAIN_TRACING_V2=true\n",
    "# export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# export LANGCHAIN_API_KEY='ls__2b782c568b874436b35388fd810a13bcx'\n",
    "# export LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to \"default\"\n",
    "# from langsmith import Client\n",
    "# from langsmith.utils import LangSmithError\n",
    "\n",
    "# client = Client()\n",
    "# dataset_name = \"RGPD test\"\n",
    "\n",
    "# # dataset creation\n",
    "# from langsmith import Client\n",
    "# from langsmith.utils import LangSmithError\n",
    "\n",
    "# client = Client()\n",
    "# dataset_name = \"NYC test\"\n",
    "\n",
    "# try:\n",
    "#     # check if dataset exists\n",
    "#     dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "#     print(\"using existing dataset: \", dataset.name)\n",
    "# except LangSmithError:\n",
    "#     # if not create a new one with the generated query examples\n",
    "#     dataset = client.create_dataset(\n",
    "#         dataset_name=dataset_name, description=\"RGPD test dataset\"\n",
    "#     )\n",
    "#     for q in Questions:\n",
    "#         client.create_example(\n",
    "#             inputs={\"query\": q},\n",
    "#             dataset_id=dataset.id,\n",
    "#         )\n",
    "\n",
    "#     print(\"Created a new dataset: \", dataset.name)\n",
    "    \n",
    "    \n",
    "# from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "# evaluation_config = RunEvalConfig(\n",
    "#     custom_evaluators=[eval_chains.values()],\n",
    "#     prediction_key=\"result\",\n",
    "# )\n",
    "    \n",
    "# result = run_on_dataset(\n",
    "#     client,\n",
    "#     dataset_name,\n",
    "#     create_qa_chain,\n",
    "#     evaluation=evaluation_config,\n",
    "#     input_mapper=lambda x: x,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.append(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
