{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "#from ragas.langchain import RagasEvaluatorChain\n",
    "# from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from test_chains import kwf_chain\n",
    "from paths import get_paths\n",
    "from config import LlmParam\n",
    "#ChatParam\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_FILE = \"llama-2-7b-chat.Q5_K_M.gguf\"\n",
    "MISTRAL_FILE = \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\"\n",
    "\n",
    "paths_dir = get_paths(model_file=LLAMA_FILE, model_name=\"LLAMA2\")\n",
    "mistral_paths = get_paths(model_file=MISTRAL_FILE, model_name=\"MISTRAL\")\n",
    "PROJECT_DIR = paths_dir[\"PROJECT_DIR\"]\n",
    "DATA_DIR = paths_dir[\"DATA_DIR\"]\n",
    "LLAMA_DIR = paths_dir[\"MODEL_DIR\"]\n",
    "MISTRAL_DIR = mistral_paths[\"MODEL_DIR\"]\n",
    "CHROMA_VECTOR_DIR = paths_dir[\"CHROMA_VECTOR_DIR\"]\n",
    "PREPARED_CHUNK_DIR = paths_dir[\"PREPARED_CHUNK_DIR\"]\n",
    "VECTOR_STORE = paths_dir[\"VECTOR_STORE\"]\n",
    "TEMPERATURE = LlmParam.TEMPERATURE\n",
    "N_GPU_LAYERS = LlmParam.N_GPU_LAYERS\n",
    "N_TOKENS = LlmParam.N_TOKENS\n",
    "KWF_TEMPLATE = LlmParam.KWF_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = '../data/BaseEvaluations.xlsx'\n",
    "eval_df = pd.read_excel(base_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"Question expert\"] = eval_df[\"Question expert\"].astype(str)\n",
    "eval_df[\"Réponse expert\"] = eval_df[\"Réponse expert\"].astype(str)\n",
    "eval_df[\"Passage\"] = eval_df[\"Passage\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_experte = eval_df[\"Question expert\"] + eval_df[\"Mots clés Question Expert\"]\n",
    "reponses_experte = eval_df[\"Réponse expert\"]\n",
    "context = eval_df[\"Passage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cuda\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for question in questions_experte:\n",
    "\ti = i+1\n",
    "\tif i <=10:\n",
    "\t\tprint(f\"Traitement de la question {i}: {question} ...\")\n",
    "\t\t#question = str(question)\n",
    "\t\tkeyword_part = question.split(\"?\")[-1]\n",
    "\t\tlogging.info(f\"Mots imposés: {keyword_part} ...\")\n",
    "\t\tmemory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\")\n",
    "\t\tprompt = PromptTemplate(\n",
    "\t\t\t\t\t\t\t\t\tinput_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "\t\t\t\t\t\t\t\t\ttemplate=KWF_TEMPLATE,\n",
    "\t\t\t\t\t)\n",
    "\t\tchain = kwf_chain(\n",
    "\t\t\t\t\t\tkeyword_part, prompt, memory, embedding,\n",
    "\t\t\t\t\t\tPREPARED_CHUNK_DIR,\n",
    "\t\t)\n",
    "\t\tinput_structure = {\"question\": question}\n",
    "\n",
    "\t\tresponse = chain.invoke(input_structure)\n",
    "\t\tanswers.append(response[\"answer\"])\n",
    "\n",
    "\t\t#exit()\n",
    "\t\t#chunk[\"source_documents\"][i].page_content\n",
    "\t\tcontexts.append([context.page_content for context in response[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"quetions\":questions_experte, \"reponses\": answers, \"ground_truth\": reponses_experte, \"context\": contexts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"questions_experte_response.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthodes Statistiques \n",
    "## Métriques basées sur la comparaison des mots ou suite de  mots (n-grams)\n",
    "- Score `BLEU`\n",
    "- Score `ROUGE` \n",
    "- Score `METEOR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"Le soleil brille dans le ciel bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = \"L'été est ma saison préférée car il fait chaud et ensoleillé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "scores_bleu = bleu.compute(predictions=[prediction], references=[reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Score  | Réference          | Prédiction |\n",
    "| :--------------- |:---------------:| -----:|\n",
    "|1  |   Le soleil brille dans le ciel bleu       |  Le soleil brille dans le ciel bleu |\n",
    "| 0  | Le soleil brille dans le ciel bleu             |   Le ciel est couvert et il pleut |\n",
    "|  0 | Le soleil brille dans le ciel bleu         |    L'été est ma saison préférée car il fait chaud et ensoleillé |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "scores_rouge = rouge.compute(predictions=[prediction], references=[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = \"Le ciel est couvert et il pleut\"\n",
    "pred2 = \"L'été est ma saison préférée car il fait chaud et ensoleillé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rouge1 = rouge.compute(predictions=[pred1], references=[reference])\n",
    "scores_rouge2 = rouge.compute(predictions=[pred2], references=[reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rouge1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rouge2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Score  | Réference          | Prédiction |\n",
    "| :--------------- |:---------------:| -----:|\n",
    "|rouge1 1, rouge2 1, rougeL 1  |   Le soleil brille dans le ciel bleu       |  Le soleil brille dans le ciel bleu |\n",
    "| rouge1 0.28, rouge2 0.16, rougeL 0.28  | Le soleil brille dans le ciel bleu             |   Le ciel est couvert et il pleut |\n",
    "|  rouge1 0, rouge2 0, rougeL 0 | Le soleil brille dans le ciel bleu         |    L'été est ma saison préférée car il fait chaud et ensoleillé |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "wer_score = wer.compute(predictions=[prediction], references=[reference])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"Le chat est sur le tapis\"\n",
    "pred1 = \"Le chat repose sur le tapis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_score = meteor.compute(predictions=[pred1], references=[reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"Le soleil brille dans le ciel bleu\"\n",
    "p2 = \"L'été est ma saison préférée car il fait chaud et ensoleillé\"\n",
    "p3 =  \"Le ciel est couvert et il pleut\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_score = meteor.compute(predictions=[p3], references=[p1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Score  | Réference          | Prédiction |\n",
    "| :--------------- |:---------------:| -----:|\n",
    "|0.99  |   Le chat est sur le tapis       |  Le chat est sur le tapis |\n",
    "| 0.8  | Le chat est sur le tapis             |   Le chat repose sur le tapis |\n",
    "|  0 | Le soleil brille dans le ciel bleu         |    L'été est ma saison préférée car il fait chaud et ensoleillé |\n",
    "|  0 | Le soleil brille dans le ciel bleu         |    Le ciel est couvert et il pleut|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métriques basées sur des modèles d'embedding\n",
    "- `BertScore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_1 = \"Les chats dorment dans le jardin\"\n",
    "p1_2 = \"Les chats se reposent dans le jardin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score = bert.compute(predictions=[p1_1], references=[p1_1], lang=\"fr\") #rescale_with_baseline=True) #model=\"microsoft/deberta-xlarge-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Score avec l'option `rescale_with_baseline=False`  | Réference          | Prédiction |\n",
    "| :--------------- |:---------------:| -----:|\n",
    "|1  |   Les chats dorment dans le jardin       |  Les chats dorment dans le jardin |\n",
    "| 0.87 | Le chat est sur le tapis             |   Les chats se reposent dans le jardin |\n",
    "|  0.78 | Il faut se conformer au RGPD         |    On est obligé d'être en règle par rapport au RGPD|\n",
    "|  0.71 | ll faut se conformer au RGPD          |   Les chats se reposent dans le jardin |\n",
    "|  0.69 | ll faut se conformer au RGPD          |    Le Gernica a été peint par Picasso|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"Le Gernica a été peint par Picasso\" #Il est important d'être conforme au RGPD\" #Il faut se conformer au réglement général de protection des données\" #\"Une sanction d'un million d'euros peut s'appliquer en cas de non conformité avec le RG\"\n",
    "#reference = \"Il faut se conformer au RGPD\" #Une sanction d'un million d'euros peut s'appliquer en cas de non conformité avec le RG\"\n",
    "predictions = \"On est obligé d'être en règle par rapport au RGPD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref1 = \"Les chats dorment dans le jardin\"\n",
    "ref2 = \"Le chat est sur le tapis\"\n",
    "ref3 = \"Il faut se conformer au RGPD\"\n",
    "pred1  = \"Les chats se reposent dans le jardin\"\n",
    "pred2 = \"On est obligé d'être en règle par rapport au RGPD\"\n",
    "pred3 = \"Les chats se reposent dans le jardin\"\n",
    "pred4 = \"Le Gernica a été peint par Picasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = bert.compute(predictions=[pred4], references=[ref3], lang=\"fr\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Score avec l'option `rescale_with_baseline=True`  | Réference          | Prédiction |\n",
    "| :--------------- |:---------------:| -----:|\n",
    "|1  |   Les chats dorment dans le jardin       |  Les chats dorment dans le jardin |\n",
    "| 0.44 | Le chat est sur le tapis             |   Les chats se reposent dans le jardin |\n",
    "|  0.43 | Il faut se conformer au RGPD         |    On est obligé d'être en règle par rapport au RGPD|\n",
    "|  0.23 | ll faut se conformer au RGPD          |   Les chats se reposent dans le jardin |\n",
    "|  0.17 | ll faut se conformer au RGPD          |    Le Gernica a été peint par Picasso|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"hello there\", \"general kenob\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "results = bleurt.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation sur les réponses du RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = pd.read_csv(\"questions_experte_response.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metriques = {\"bleu\": evaluate.load('bleu'),\n",
    "             'rouge': evaluate.load('rouge'), \n",
    "             'meteor': evaluate.load('meteor'),\n",
    "             'bert': evaluate.load('bertscore')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references  = test_eval['ground_truth'].values\n",
    "predictions = test_eval['reponses'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleuScore = []\n",
    "rougeScore = []\n",
    "meteorScore = []\n",
    "bertScore = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = test_eval[\"ground_truth\"]\n",
    "predictions = test_eval[\"reponses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleuscore = []\n",
    "rougescore = []\n",
    "meteorscore = []\n",
    "bertscore = []\n",
    "for i in range(len(references)):\n",
    " bleu = metriques[\"bleu\"].compute(references=[references[i]], predictions=[predictions[i]])\n",
    " rouge = metriques[\"rouge\"].compute(references=[references[i]], predictions=[predictions[i]])\n",
    " meteor = metriques[\"meteor\"].compute(references=[references[i]], predictions=[predictions[i]])\n",
    " bert = metriques[\"bert\"].compute(references=[references[i]], predictions=[predictions[i]], lang=\"fr\")\n",
    " bleuscore.append(bleu[\"bleu\"])\n",
    " rougescore.append({\"rouge1\": rouge[\"rouge1\"], \"rouge2\" : rouge[\"rouge2\"], \"rougeL\": rouge[\"rougeL\"]})\n",
    " meteorscore.append(meteor[\"meteor\"])\n",
    " bertscore.append(bert[\"precision\"][0])\n",
    " #print(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleuscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rougescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval[\"bleuScore\"] = bleuscore\n",
    "test_eval[\"rougeScore\"] = rougescore\n",
    "test_eval[\"meteorScore\"] = meteorscore\n",
    "test_eval[\"bertScore\"] = bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval.to_excel(\"EvaluationStatisquesEmbedding.xlsx\" , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FrameWork DeepEval un equivalent de pytest pour les LLMs \n",
    "- GEval Evaluation par LLM: Evaluation par gpt par défaut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Les administrations de plus de cinquante employés sont obligées de publier en ligne non seulement les documents administratifs communiqués selon le CRPA, mais aussi ceux inclus dans le Répertoire des Informations Publiques (RIP), ainsi que les bases de données et les données régulièrement mises à jour qui ont un intérêt économique, social, sanitaire ou environnemental\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"Les articles L. 312-1-1 et L. 312-1-3 du CRPA imposent aux administrations de mettre en ligne des documents administratifs sur leur site internet, dans les délais prévus par la législation. Ces obligations s'appliquent aux administrations de plus de cinquante employés, ainsi qu'aux collectivités territoriales et aux établissements publics de coopération intercommunale. Les documents administratifs doivent être mis en ligne dans un délai de trente jours à compter de la date de leur création, sauf dans les cas où une mise en ligne plus rapide est nécessaire pour répondre à une demande expresse de communication. Les administrations doivent également mettre en ligne les documents qui leur sont demandés, même s'ils n'ont pas encore été créés, dans un délai de vingt jours à compter de la date de la demande. Enfin, les administrations doivent mettre en ligne les documents qui leur sont communiqués par les autres administrations, dans un délai de vingt jours à compter de la date de la communication. Les obligations de mise en ligne sont prévues par les articles L. 312-1-1 et L. 312-1-3 du CRPA, et sont applicables aux administrations de plus de cinquante employés, ainsi qu'aux collectivités territoriales et aux établissements publics de coopération intercommunale. Dispose d’un menu contextuel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class LLAMA2(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"LLAMA 2\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf_model = \"meta-llama/Llama-2-70b-chat\" #mistralai/Mistral-7B-v0.1\" #\"meta-llama/Llama-2-70b-chat\" \n",
    "#hf_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "hf_model = \"meta-llama/Llama-2-7b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\n",
    "llama2 = LLAMA2(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(input=input, actual_output=output)\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Coherence - la qualité collective de toutes les phrases dans la sortie acutuelle\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=llama2\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score)\n",
    "print(coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
